#!/usr/bin/env python3"""Check computer for missing audio datasets."""import osimport sysfrom pathlib import Pathfrom collections import defaultdict# Try to use configs/storage.py for path detectiontry:    from configs.storage import get_storage_config    _use_storage_config = Trueexcept ImportError:    _use_storage_config = Falsedef get_locations():    """Get all potential locations for datasets using environment variables and storage config."""    locations = []    # 1. Use storage config if available    if _use_storage_config:        try:            config = get_storage_config()            if config.audio_data_root.exists():                locations.append(str(config.audio_data_root))        except Exception:            pass    # 2. Environment variables (highest priority if not using storage config)    for env_var in ["KELLY_AUDIO_DATA_ROOT", "AUDIO_DATA_ROOT", "KMIDI_DATA_DIR"]:        env_path = os.environ.get(env_var)        if env_path:            path = Path(env_path).expanduser()            if path.exists() and path not in [Path(l) for l in locations]:                locations.append(str(path))    # 3. Platform-specific defaults (fallbacks only)    home = Path.home()    platform_defaults = [        # Primary location (files moved from SSD)        home / "RECOVERY_OPS" / "AUDIO_MIDI_DATA" / "kelly-audio-data",        # Alternative local locations        home / "audio",        home / "Music" / "AudioVault",        home / ".kelly" / "audio-data",        home / "BASIC STRUCTURE FOR miDiKompanion",    ]    # Add platform defaults that exist    for path in platform_defaults:        if path.exists() and str(path) not in locations:            locations.append(str(path))    # 4. Legacy SSD locations (kept for backward compatibility if remounted)    legacy_ssd_paths = [        "/Volumes/Extreme SSD/kelly-audio-data",        "/Volumes/sbdrive/kmidi_audio_data",        "/Volumes/Extreme SSD/kmidi_audio_data",    ]    for path_str in legacy_ssd_paths:        path = Path(path_str)        if path.exists() and str(path) not in locations:            locations.append(str(path))    return locations# Check all potential locations# Updated: Files moved from external SSD to local storage (2025-01-09)# Uses environment variables and configs/storage.py for path detectionlocations = get_locations()datasets_to_check = {    "MAESTRO": ["raw/melodies/maestro", "raw/melodies/maestro-v3.0.0", "melodies/maestro"],    "RAVDESS": ["raw/emotions/ravdess", "emotions/ravdess"],    "CREMA-D": ["raw/emotions/cremad", "emotions/cremad"],    "TESS": ["raw/emotions/tess", "emotions/tess"],    "GTZAN": ["raw/emotions/gtzan", "emotions/gtzan"],    "Groove MIDI": ["raw/grooves/groove_midi", "grooves/groove_midi"],    "Lakh MIDI": ["raw/chord_progressions/lakh", "raw/chord_progressions/lmd_matched", "chord_progressions/lakh"],    "MusicNet": ["raw/melodies/musicnet", "melodies/musicnet"],}results = {}for base_path in locations:    base = Path(base_path)    if not base.exists():        continue    print(f"\n{'='*60}")    print(f"Checking: {base_path}")    print(f"{'='*60}")    for dataset_name, paths in datasets_to_check.items():        for rel_path in paths:            full_path = base / rel_path            if full_path.exists() and full_path.is_dir():                try:                    # Count files                    all_files = [f for f in full_path.rglob("*") if f.is_file()]                    file_count = len(all_files)                    # Calculate size                    total_size = sum(f.stat().st_size for f in all_files) / (1024**3)                    key = f"{base_path}::{dataset_name}"                    if key not in results or results[key]["files"] < file_count:                        results[key] = {                            "path": str(full_path),                            "files": file_count,                            "size_gb": total_size                        }                        print(f"  ✓ {dataset_name}: {file_count:,} files ({total_size:.2f} GB) at {rel_path}")                except Exception as e:                    print(f"  ⚠ {dataset_name}: Error accessing {rel_path}: {e}")print("\n" + "="*60)print("FOUND DATASETS SUMMARY")print("="*60)# Group by dataset namefound = defaultdict(list)for key, info in results.items():    dataset = key.split("::")[1]    found[dataset].append(info)missing = []for dataset_name in sorted(datasets_to_check.keys()):    if dataset_name in found:        print(f"\n✓ {dataset_name}: FOUND")        for info in found[dataset_name]:            print(f"    Location: {info['path']}")            print(f"    Files: {info['files']:,}")            print(f"    Size: {info['size_gb']:.2f} GB")    else:        print(f"\n❌ {dataset_name}: NOT FOUND")        missing.append(dataset_name)print("\n" + "="*60)print("MISSING DATASETS")print("="*60)for name in missing:    print(f"  - {name}")# Also check user's audio directory structure if it existsaudio_dir = Path.home() / "audio"if audio_dir.exists():    print("\n" + "="*60)    print(f"CHECKING {audio_dir}")    print("="*60)    print(f"Directory exists: {audio_dir}")    try:        subdirs = [d for d in audio_dir.iterdir() if d.is_dir()]        print(f"Subdirectories: {len(subdirs)}")        for subdir in sorted(subdirs)[:20]:            try:                files = list(subdir.rglob("*"))                file_count = len([f for f in files if f.is_file()])                if file_count > 0:                    size = sum(f.stat().st_size for f in files if f.is_file()) / (1024**3)                    print(f"  {subdir.name}: {file_count:,} files ({size:.2f} GB)")            except:                pass    except Exception as e:        print(f"Error checking directory: {e}")else:    print("Directory does not exist")